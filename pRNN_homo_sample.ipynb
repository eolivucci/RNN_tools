{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pRNN wavefunction with Constrained Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a class $\\textrm{RNNWavefunction(tf.keras.Model)}$ for sampling monomials of homogeneous degree in a given number M of variables $(x_1,x_2,...,x_M)$. \n",
    "The homogeneity constraint does not spoils the auto-regressive property of RNNs. It is implemented generalizing the scheme of $\\textit{Appendix D}$ of $\\textrm{[PhysRevResearch.2.023358]}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pRNN wavefunction ansazt with GRU layer + Dense softmax\n",
    "# Samples homogeneous monomials in Nc^2 and fixed degree \n",
    "# Needs to be fixed by broadcasting\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class RNNWavefunction(tf.keras.Model):\n",
    "    def __init__(self, system_size, units=20, input_dim=3, output_dim=3, seed=211):\n",
    "        \"\"\"\n",
    "        system_size: int, number of timesteps or system size (= number of monomial variables)\n",
    "        units: int, number of units in the GRU layer\n",
    "        input_dim: int, number of input features (= monomial homogeneous degree = charge+1)\n",
    "        output_dim: int, number of output features (= monomial homogeneous degree = charge+1)\n",
    "        seed: int, the random seed for reproducibility\n",
    "        \"\"\"\n",
    "        super(RNNWavefunction, self).__init__()\n",
    "\n",
    "        # Set random seeds for reproducibility\n",
    "        np.random.seed(seed)\n",
    "        tf.random.set_seed(seed)\n",
    "\n",
    "        self.system_size = system_size\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        # Define the GRU layer (one GRU layer with specified units)\n",
    "        self.gru = tf.keras.layers.GRU(units=units, return_sequences=True, return_state=True)\n",
    "        \n",
    "        # Final Dense layer with Softmax output (probabilities, not logits!)\n",
    "        self.dense = tf.keras.layers.Dense(output_dim, activation=\"softmax\")\n",
    "        \n",
    "    def call(self, inputs, hidden_state=None, training=False):\n",
    "        \"\"\"\n",
    "        Forward pass through the network with fixed hidden state.\n",
    "        \"\"\"\n",
    "        print(\"Now input is: \"+str(inputs))\n",
    "        if hidden_state is None:\n",
    "            hidden_state = tf.zeros((inputs.shape[0], self.gru.units))  # Fixed hidden state\n",
    "\n",
    "        x, hidden_state = self.gru(inputs, initial_state=hidden_state, training=training)  # GRU layer\n",
    "        x = self.dense(x)  # Apply Dense layer\n",
    "\n",
    "        print(\"result of softmax = \"+str(x))\n",
    "        return x, hidden_state\n",
    "    \n",
    "    def sample(self, numsamples):\n",
    "\n",
    "        \"\"\"\n",
    "        Generate samples from the probability distribution parameterized by the RNN.\n",
    "        numsamples: int, number of samples to generate\n",
    "        \"\"\"\n",
    "        samples = []  # List to store the generated sequence\n",
    "        inputs = tf.zeros((numsamples, 1, self.input_dim), dtype=tf.float32)  # Initial input (zero vector)\n",
    "        hidden_state = None  # No initial hidden state\n",
    "\n",
    "        for t in range(self.system_size-1): # Sampling cycle over system_size =  number of variables\n",
    "            output, hidden_state = self.call(inputs, hidden_state=hidden_state)  # Forward pass through the model\n",
    "            #print(\"At step t=\"+str(t)+\" the output is: \"+str(output))\n",
    "            # Get probabilities for the last generated timestep\n",
    "            softout= output[:, -1, :]  # Shape: [numsamples, output_dim]\n",
    "            \n",
    "            # Projection of softmax probabilities imposing charge conservation\n",
    "            # At each step applied a mask thetavec that evaluates to zero over all the samples that exceed the total charge\n",
    "            softout_t = np.copy(softout)\n",
    "            thetavec = [np.heaviside(self.input_dim-1-np.sum(np.array(samples),axis=0)-i,1) for i in range(softout_t.shape[1])] \n",
    "            softout_t = np.array([softout_t[:,i]*thetavec[i] for i in range(softout_t.shape[1])])\n",
    "            norm = np.sum(softout_t,axis=0) \n",
    "            softout_t = softout_t/norm # Re-normalize the masked softmax output\n",
    "\n",
    "            softout_t = np.transpose(softout_t)\n",
    "            print(\"At step t=\"+str(t)+\" the softmax probs are: \"+str(softout_t))\n",
    "            # Sample from categorical distribution\n",
    "        \n",
    "            sampled_t = tf.random.categorical(tf.math.log(softout_t), num_samples=1)  # Shape: [numsamples, 1]\n",
    "            sampled_t = tf.squeeze(sampled_t, axis=-1)  # Shape: [numsamples]\n",
    "\n",
    "            print(\"The new sampled site is \"+str(sampled_t)+\" at time step =\"+str(t))\n",
    "            # Append sampled values to the list\n",
    "            samples.append(sampled_t)\n",
    "        \n",
    "            print(\"At time step \"+str(t)+\" the sample is =\"+str( tf.stack(samples, axis=1)))\n",
    "            # Convert sampled values to one-hot encoding for the next input\n",
    "            inputs = tf.one_hot(sampled_t, depth=self.output_dim, dtype=tf.float32)\n",
    "            #print(\"Ready to expand the new input \"+str(inputs)+\" at time step =\"+str(t))\n",
    "            inputs = tf.expand_dims(inputs, axis=1)  # Add time-step dimension\n",
    "            \n",
    "        samples = tf.stack(samples, axis=1)\n",
    "\n",
    "        # Add last system site to the sample, fixed by charge conservation\n",
    "        J = tf.constant((self.input_dim-1)*np.ones(samples.shape[0])-np.sum(samples,axis=1))\n",
    "        J = np.transpose(tf.cast(tf.expand_dims(J, axis=0), dtype = tf.int64))\n",
    "        #print(samples)\n",
    "        #print(J)\n",
    "        samples = tf.concat([samples, J], axis=1)\n",
    "\n",
    "        return samples\n",
    "\n",
    "    def log_probability(self, samples):\n",
    "        \"\"\"\n",
    "        Calculate log-probabilities of the given samples.\n",
    "        samples: Tensor, shape (numsamples, system_size), the sampled wavefunction\n",
    "        \"\"\"\n",
    "        # Convert samples to one-hot encoding\n",
    "\n",
    "        \n",
    "\n",
    "        one_hot_samples = tf.one_hot(samples, depth=self.output_dim, dtype=tf.float32)\n",
    "\n",
    "        inputs = one_hot_samples  # Shape: [numsamples, system_size, output_dim]\n",
    "    \n",
    "        # Ensure evaluation mode (training=False)\n",
    "        probs, _ = self.call(inputs, training=False)  # Forward pass through the model with training=False\n",
    "        print(probs)\n",
    "        \n",
    "        # Compute log probabilities (log(p(x)))\n",
    "        log_probs = tf.reduce_sum(tf.math.log(tf.reduce_sum(tf.multiply(probs, one_hot_samples), axis=-1)), axis=-1)\n",
    "\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now input is: tf.Tensor([[[0. 0.]]], shape=(1, 1, 2), dtype=float32)\n",
      "result of softmax = tf.Tensor([[[0.5 0.5]]], shape=(1, 1, 2), dtype=float32)\n",
      "At step t=0 the softmax probs are: [[0.5 0.5]]\n",
      "The new sampled site is tf.Tensor([1], shape=(1,), dtype=int64) at time step =0\n",
      "At time step 0 the sample is =tf.Tensor([[1]], shape=(1, 1), dtype=int64)\n",
      "Now input is: tf.Tensor([[[0. 1.]]], shape=(1, 1, 2), dtype=float32)\n",
      "result of softmax = tf.Tensor([[[0.5265186  0.47348142]]], shape=(1, 1, 2), dtype=float32)\n",
      "At step t=1 the softmax probs are: [[1. 0.]]\n",
      "The new sampled site is tf.Tensor([0], shape=(1,), dtype=int64) at time step =1\n",
      "At time step 1 the sample is =tf.Tensor([[1 0]], shape=(1, 2), dtype=int64)\n",
      "Now input is: tf.Tensor([[[1. 0.]]], shape=(1, 1, 2), dtype=float32)\n",
      "result of softmax = tf.Tensor([[[0.46289626 0.5371037 ]]], shape=(1, 1, 2), dtype=float32)\n",
      "At step t=2 the softmax probs are: [[1. 0.]]\n",
      "The new sampled site is tf.Tensor([0], shape=(1,), dtype=int64) at time step =2\n",
      "At time step 2 the sample is =tf.Tensor([[1 0 0]], shape=(1, 3), dtype=int64)\n",
      "Generated Samples:\n",
      "[[1 0 0 0]]\n",
      "Now input is: tf.Tensor(\n",
      "[[[0. 1.]\n",
      "  [1. 0.]\n",
      "  [1. 0.]\n",
      "  [1. 0.]]], shape=(1, 4, 2), dtype=float32)\n",
      "result of softmax = tf.Tensor(\n",
      "[[[0.5265186  0.47348142]\n",
      "  [0.46289626 0.5371037 ]\n",
      "  [0.42690367 0.57309633]\n",
      "  [0.40818754 0.59181243]]], shape=(1, 4, 2), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[[0.5265186  0.47348142]\n",
      "  [0.46289626 0.5371037 ]\n",
      "  [0.42690367 0.57309633]\n",
      "  [0.40818754 0.59181243]]], shape=(1, 4, 2), dtype=float32)\n",
      "Generated Samples:\n",
      "[-3.2651205]\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "Nc = 2 \n",
    "system_size = Nc*Nc # Number of timesteps = # of variables M\n",
    "charge = 1 # Polynomial Degree\n",
    "input_dim = charge+1  # Number of input features = maximal number of excitations = exponents (includes 0)\n",
    "output_dim = charge+1   # Number of output classes = maximal number of excitations = exponents (includes 0)\n",
    "\n",
    "# GRU units\n",
    "units = 10\n",
    "numsamples = 1  # Number of samples to generate\n",
    "\n",
    "# Instantiate the RNNWavefunction model\n",
    "model = RNNWavefunction(system_size, units, input_dim, output_dim, seed=np.random.randint(1,200))\n",
    "\n",
    "# Example: Sampling\n",
    "samples = model.sample(numsamples)\n",
    "print(f\"Generated Samples:\\n{samples.numpy()[:10]}\")\n",
    "\n",
    "# Example: Evaluation\n",
    "log_probs = model.log_probability(samples)\n",
    "print(f\"Generated Samples:\\n{log_probs.numpy()[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
