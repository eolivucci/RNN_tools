{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pRNN wavefunction with Constrained Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a class $\\textrm{RNNWavefunction(tf.keras.Model)}$ for sampling monomials of homogeneous degree in a given number M of variables $(x_1,x_2,...,x_M)$. \n",
    "The homogeneity constraint does not spoils the auto-regressive property of RNNs and it is inspired to the prescription of Appendix D of [PhysRevResearch.2.023358]. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pRNN wavefunction ansazt with GRU layer + Dense softmax\n",
    "# Samples homogeneous monomials in Nc^2 and fixed degree \n",
    "# Needs to be fixed by broadcasting\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class RNNWavefunction(tf.keras.Model):\n",
    "    def __init__(self, system_size, units=20, input_dim=3, output_dim=3, seed=211):\n",
    "        \"\"\"\n",
    "        system_size: int, number of timesteps or system size (= number of monomial variables)\n",
    "        units: int, number of units in the GRU layer\n",
    "        input_dim: int, number of input features (= monomial homogeneous degree = charge+1)\n",
    "        output_dim: int, number of output features (= monomial homogeneous degree = charge+1)\n",
    "        seed: int, the random seed for reproducibility\n",
    "        \"\"\"\n",
    "        super(RNNWavefunction, self).__init__()\n",
    "\n",
    "        # Set random seeds for reproducibility\n",
    "        np.random.seed(seed)\n",
    "        tf.random.set_seed(seed)\n",
    "\n",
    "        self.system_size = system_size\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        # Define the GRU layer (one GRU layer with specified units)\n",
    "        self.gru = tf.keras.layers.GRU(units=units, return_sequences=True, return_state=True)\n",
    "        \n",
    "        # Final Dense layer with Softmax output (probabilities, not logits!)\n",
    "        self.dense = tf.keras.layers.Dense(output_dim, activation=\"softmax\")\n",
    "        \n",
    "    def call(self, inputs, hidden_state=None, training=False):\n",
    "        \"\"\"\n",
    "        Forward pass through the network with fixed hidden state.\n",
    "        \"\"\"\n",
    "        if hidden_state is None:\n",
    "            hidden_state = tf.zeros((inputs.shape[0], self.gru.units))  # Fixed hidden state\n",
    "\n",
    "        x, hidden_state = self.gru(inputs, initial_state=hidden_state, training=training)  # GRU layer\n",
    "        x = self.dense(x)  # Apply Dense layer\n",
    "\n",
    "        return x, hidden_state\n",
    "\n",
    "    def sample(self, numsamples):\n",
    "\n",
    "        \"\"\"\n",
    "        Generate samples from the probability distribution parameterized by the RNN.\n",
    "        numsamples: int, number of samples to generate\n",
    "        \"\"\"\n",
    "        samples = []  # List to store the generated sequence\n",
    "        inputs = tf.zeros((numsamples, 1, self.input_dim), dtype=tf.float32)  # Initial input (zero vector)\n",
    "        hidden_state = None  # No initial hidden state\n",
    "\n",
    "        for t in range(self.system_size-1): # Sampling cycle over system_size =  number of variables\n",
    "            output, hidden_state = self.call(inputs, hidden_state=hidden_state)  # Forward pass through the model\n",
    "\n",
    "            # Get probabilities for the last generated timestep\n",
    "            softout= output[:, -1, :]  # Shape: [numsamples, output_dim]\n",
    "            #print(\"Sample is now = \"+str(samples))\n",
    "            # Projection of softmax probabilities imposing charge conservation\n",
    "            softout_t = np.copy(softout)\n",
    "            #print(\"Starting with softout_t = \"+str(softout))\n",
    "            thetavec = [np.heaviside(self.input_dim-1-np.sum(np.array(samples),axis=0)-i,1) for i in range(softout_t.shape[1])]\n",
    "            #print(\"Apply projection thetavec = \"+str(thetavec))\n",
    "            softout_t = np.array([softout_t[:,i]*thetavec[i] for i in range(softout_t.shape[1])])\n",
    "\n",
    "            #norms = np.sum(softout_t,axis=0)\n",
    "            #print(\"Normalize = \"+str(softout_t))\n",
    "            #print(\"by norm vec = \"+str(norms))\n",
    "            softout_t = np.transpose(softout_t)\n",
    "            #print(softout_t / np.sum(softout_t, axis=1, keepdims=True))\n",
    "            \n",
    "\n",
    "            # Sample from categorical distribution\n",
    "        \n",
    "            sampled_t = tf.random.categorical(tf.math.log(softout_t), num_samples=1)  # Shape: [numsamples, 1]\n",
    "            sampled_t = tf.squeeze(sampled_t, axis=-1)  # Shape: [numsamples]\n",
    "        \n",
    "            # Append sampled values to the list\n",
    "            samples.append(sampled_t)\n",
    "        \n",
    "            # Convert sampled values to one-hot encoding for the next input\n",
    "            inputs = tf.one_hot(sampled_t, depth=self.output_dim, dtype=tf.float32)\n",
    "            inputs = tf.expand_dims(inputs, axis=1)  # Add time-step dimension\n",
    "            \n",
    "        samples = tf.stack(samples, axis=1)\n",
    "\n",
    "        J = tf.constant((self.input_dim-1)*np.ones(samples.shape[0])-np.sum(samples,axis=1))\n",
    "        J = np.transpose(tf.cast(tf.expand_dims(J, axis=0), dtype = tf.int64))\n",
    "        #print(samples)\n",
    "        #print(J)\n",
    "        samples = tf.concat([samples, J], axis=1)\n",
    "\n",
    "        return samples\n",
    "\n",
    "    def log_probability(self, samples):\n",
    "        \"\"\"\n",
    "        Calculate log-probabilities of the given samples.\n",
    "        samples: Tensor, shape (numsamples, system_size), the sampled wavefunction\n",
    "        \"\"\"\n",
    "        # Convert samples to one-hot encoding\n",
    "        one_hot_samples = tf.one_hot(samples, depth=self.output_dim, dtype=tf.float32)\n",
    "\n",
    "        inputs = one_hot_samples  # Shape: [numsamples, system_size, output_dim]\n",
    "    \n",
    "        # Ensure evaluation mode (training=False)\n",
    "        probs, _ = self.call(inputs, training=False)  # Forward pass through the model with training=False\n",
    "        \n",
    "        # Compute log probabilities (log(p(x)))\n",
    "        log_probs = tf.reduce_sum(tf.math.log(tf.reduce_sum(tf.multiply(probs, one_hot_samples), axis=-1)), axis=-1)\n",
    "\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 4  5  1  0]\n",
      " [ 5  4  0  1]\n",
      " [ 1  3  0  6]\n",
      " [ 7  2  1  0]\n",
      " [ 3  4  3  0]\n",
      " [ 3  5  1  1]\n",
      " [10  0  0  0]\n",
      " [ 2  5  1  2]\n",
      " [ 4  4  2  0]\n",
      " [ 5  0  1  4]], shape=(10, 4), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "Nc = 2  \n",
    "system_size = Nc*Nc # Number of timesteps\n",
    "charge = 10 # Polynomial Degree\n",
    "input_dim = charge+1  # Number of input features = total charge\n",
    "output_dim = charge+1   # Number of output classes = total charge\n",
    "units = 1     # Number of\n",
    "# GRU units\n",
    "numsamples = 10  # Number of samples to generate\n",
    "seed = 221      # Random seed for reproducibility\n",
    "\n",
    "# Instantiate the RNNWavefunction model\n",
    "model = RNNWavefunction(system_size, units, input_dim, output_dim, seed=np.random.randint(1,100))\n",
    "\n",
    "# Example: Sampling\n",
    "samples = model.sample(numsamples)\n",
    "print(samples)\n",
    "#print(f\"Generated Samples:\\n{samples.numpy()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
